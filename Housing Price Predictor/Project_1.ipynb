{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Frame the problem\n",
    "As the contractor, I must develop an AI model that can accurately predict housing prices within in the LA area using attributes of the property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use different ML models to predict the survivability chances for a passenger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Data \n",
    "I will be scraping a website like Zillow in order to create my own data set of LA houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting data from page 1: https://www.zillow.com/los-angeles-ca/sold/\n",
      "Successfully fetched page 1.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 2: https://www.zillow.com/los-angeles-ca/sold/2_p/\n",
      "Successfully fetched page 2.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 3: https://www.zillow.com/los-angeles-ca/sold/3_p/\n",
      "Successfully fetched page 3.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 4: https://www.zillow.com/los-angeles-ca/sold/4_p/\n",
      "Successfully fetched page 4.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 5: https://www.zillow.com/los-angeles-ca/sold/5_p/\n",
      "Successfully fetched page 5.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 6: https://www.zillow.com/los-angeles-ca/sold/6_p/\n",
      "Successfully fetched page 6.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 7: https://www.zillow.com/los-angeles-ca/sold/7_p/\n",
      "Successfully fetched page 7.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 8: https://www.zillow.com/los-angeles-ca/sold/8_p/\n",
      "Successfully fetched page 8.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 9: https://www.zillow.com/los-angeles-ca/sold/9_p/\n",
      "Successfully fetched page 9.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 10: https://www.zillow.com/los-angeles-ca/sold/10_p/\n",
      "Successfully fetched page 10.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 11: https://www.zillow.com/los-angeles-ca/sold/11_p/\n",
      "Successfully fetched page 11.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 12: https://www.zillow.com/los-angeles-ca/sold/12_p/\n",
      "Successfully fetched page 12.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 13: https://www.zillow.com/los-angeles-ca/sold/13_p/\n",
      "Successfully fetched page 13.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 14: https://www.zillow.com/los-angeles-ca/sold/14_p/\n",
      "Successfully fetched page 14.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 15: https://www.zillow.com/los-angeles-ca/sold/15_p/\n",
      "Successfully fetched page 15.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 16: https://www.zillow.com/los-angeles-ca/sold/16_p/\n",
      "Successfully fetched page 16.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 17: https://www.zillow.com/los-angeles-ca/sold/17_p/\n",
      "Successfully fetched page 17.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 18: https://www.zillow.com/los-angeles-ca/sold/18_p/\n",
      "Successfully fetched page 18.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 19: https://www.zillow.com/los-angeles-ca/sold/19_p/\n",
      "Successfully fetched page 19.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 20: https://www.zillow.com/los-angeles-ca/sold/20_p/\n",
      "Successfully fetched page 20.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 21: https://www.zillow.com/los-angeles-ca/sold/21_p/\n",
      "Successfully fetched page 21.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 22: https://www.zillow.com/los-angeles-ca/sold/22_p/\n",
      "Successfully fetched page 22.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 23: https://www.zillow.com/los-angeles-ca/sold/23_p/\n",
      "Successfully fetched page 23.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 24: https://www.zillow.com/los-angeles-ca/sold/24_p/\n",
      "Successfully fetched page 24.\n",
      "Found 41 properties on this page.\n",
      "Requesting data from page 25: https://www.zillow.com/los-angeles-ca/sold/25_p/\n",
      "Error making request on page 25: 400 Client Error: Bad Request for url: https://www.zillow.com/los-angeles-ca/sold/25_p/\n",
      "\n",
      "Finished scraping. Found a total of 984 properties.\n",
      "\n",
      "Data successfully saved to 'zillow_sold_los_angeles.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_price_to_numeric(price_str):\n",
    "    \"\"\"\n",
    "    Converts a price string (e.g., '$2.5M', '500K') to a numeric value.\n",
    "    \"\"\"\n",
    "    if price_str is None:\n",
    "        return None\n",
    "    price_str = str(price_str).strip().upper()\n",
    "    \n",
    "    if isinstance(price_str, (int, float)):\n",
    "        return price_str\n",
    "\n",
    "    price_str = price_str.replace('$', '').replace(',', '')\n",
    "    \n",
    "    if 'M' in price_str:\n",
    "        return int(float(price_str.replace('M', '')) * 1_000_000)\n",
    "    elif 'K' in price_str:\n",
    "        return int(float(price_str.replace('K', '')) * 1_000)\n",
    "    \n",
    "    try:\n",
    "        return int(price_str)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def scrape_zillow_sold_data(city='los-angeles', state='ca'):\n",
    "    \"\"\"\n",
    "    Scrapes sold housing data from Zillow for a given city and state,\n",
    "    handling pagination and extracting key features from search result pages.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"Accept-Language\": \"en\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "        \"Pragma\": \"no-cache\",\n",
    "        \"Sec-Ch-Ua\": '\"Chromium\";v=\"130\", \"Google Chrome\";v=\"130\", \"Not?A_Brand\";v=\"99\"',\n",
    "        \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "        \"Sec-Ch-Ua-Platform\": '\"Windows\"',\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
    "    }\n",
    "    \n",
    "    all_properties = []\n",
    "    page_number = 1\n",
    "\n",
    "    while True:\n",
    "        if page_number == 1:\n",
    "            url = f\"https://www.zillow.com/{city}-{state}/sold/\"\n",
    "        else:\n",
    "            url = f\"https://www.zillow.com/{city}-{state}/sold/{page_number}_p/\"\n",
    "        \n",
    "        print(f\"Requesting data from page {page_number}: {url}\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            print(f\"Successfully fetched page {page_number}.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error making request on page {page_number}: {e}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        script_tag = soup.find('script', {'id': '__NEXT_DATA__'})\n",
    "        \n",
    "        if not script_tag:\n",
    "            print(\"Could not find the data script tag. The page structure may have changed.\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            json_data = json.loads(script_tag.string)\n",
    "            search_results = json_data['props']['pageProps']['searchPageState']['cat1']['searchResults']['listResults']\n",
    "            \n",
    "            if not search_results:\n",
    "                print(\"No more properties found. Reached the last page.\")\n",
    "                break\n",
    "            \n",
    "            page_properties = []\n",
    "            for property_data in search_results:\n",
    "                hdp_data = property_data.get('hdpData', {})\n",
    "                home_info = hdp_data.get('homeInfo', {})\n",
    "                property_type = home_info.get('homeType', 'N/A')\n",
    "                \n",
    "\n",
    "                ain = property_data.get('parcelId')\n",
    "\n",
    "                page_properties.append({\n",
    "                    'Address': property_data.get('address', 'N/A'),\n",
    "                    'AIN': ain,\n",
    "                    'Sold Price': convert_price_to_numeric(property_data.get('soldPrice', property_data.get('price', 'N/A'))),\n",
    "                    'Bedrooms': property_data.get('beds'),\n",
    "                    'Bathrooms': property_data.get('baths'),\n",
    "                    'Area (SqFt)': property_data.get('area'),\n",
    "                    'Property Type': property_type,\n",
    "                })\n",
    "            \n",
    "            all_properties.extend(page_properties)\n",
    "            print(f\"Found {len(page_properties)} properties on this page.\")\n",
    "            \n",
    "            page_number += 1\n",
    "            time.sleep(2)\n",
    "\n",
    "        except (KeyError, json.JSONDecodeError) as e:\n",
    "            print(f\"Error parsing JSON data: {e}. The data structure might have changed.\")\n",
    "            break\n",
    "\n",
    "    if not all_properties:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nFinished scraping. Found a total of {len(all_properties)} properties.\")\n",
    "    return pd.DataFrame(all_properties)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraped_data = scrape_zillow_sold_data(city='los-angeles', state='ca')\n",
    "\n",
    "    if not scraped_data.empty:\n",
    "        output_filename = 'zillow_sold_los_angeles.csv'\n",
    "        scraped_data.to_csv(output_filename, index=False)\n",
    "        print(f\"\\nData successfully saved to '{output_filename}'\")\n",
    "    else:\n",
    "        print(\"\\nScraping failed or no data was found. No file was saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore the Data\n",
    "Currently I have 6 Features, the address (can get neighborhood from this), price sold, sqft, # bedrooms, # bathrooms, and property type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Address  AIN  Sold Price  \\\n",
      "0  7120 Carlson Cir UNIT 256, Canoga Park, CA 91303  NaN      380000   \n",
      "1        22410 Collins St, Woodland Hills, CA 91367  NaN     1760000   \n",
      "2                  18319 Jovan St, Reseda, CA 91335  NaN      780000   \n",
      "3          21228 Lopez St, Woodland Hills, CA 91364  NaN     1070000   \n",
      "4            4750 Poe Ave, Woodland Hills, CA 91364  NaN     2600000   \n",
      "\n",
      "   Bedrooms  Bathrooms  Area (SqFt)  Property Type  \n",
      "0       1.0        1.0        471.0          CONDO  \n",
      "1       4.0        3.0       2616.0  SINGLE_FAMILY  \n",
      "2       3.0        1.0       1398.0  SINGLE_FAMILY  \n",
      "3       4.0        2.0       1409.0  SINGLE_FAMILY  \n",
      "4       4.0        4.0       3287.0  SINGLE_FAMILY  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"zillow_sold_los_angeles.csv\")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filled in missing values for Price, bedrooms, and bathrooms with median as to not be skewed by outliers. \n",
    "One-hot Encoded property type and neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (787, 5)\n",
      "Test shape: (197, 5)\n",
      "Files saved: train_data.csv, test_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"zillow_sold_los_angeles.csv\")\n",
    "\n",
    "if \"AIN\" in df.columns:\n",
    "    df = df.drop(columns=[\"AIN\"])\n",
    "\n",
    "def extract_neighborhood(address):\n",
    "    match = re.search(r\",\\s*([^,]+), CA\", address)\n",
    "    return match.group(1).strip() if match else \"Los Angeles\"\n",
    "\n",
    "df[\"Neighborhood\"] = df[\"Address\"].apply(extract_neighborhood)\n",
    "\n",
    "target = \"Sold Price\"\n",
    "X = df.drop(columns=[target, \"Address\"])\n",
    "y = df[target]\n",
    "\n",
    "numeric_features = [\"Bedrooms\", \"Bathrooms\", \"Area (SqFt)\"]\n",
    "categorical_features = [\"Property Type\", \"Neighborhood\"]\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "train_data.to_csv(\"train_data.csv\", index=False)\n",
    "test_data.to_csv(\"test_data.csv\", index=False)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "print(\"Files saved: train_data.csv, test_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model the data\n",
    "Using selected ML models, experment with your choices and describe your findings. Finish by selecting a Model to continue with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (787, 6)\n",
      "Testing data shape: (197, 6)\n",
      "Training Random Forest model...\n",
      "Model training completed!\n",
      "Making predictions...\n",
      "\n",
      "Training Set Performance:\n",
      "  Mean Squared Error (MSE): $481,198,457,370.99\n",
      "  Root Mean Squared Error (RMSE): $693,684.70\n",
      "  Mean Absolute Error (MAE): $314,595.74\n",
      "  R² Score: 0.8470\n",
      "\n",
      "Testing Set Performance:\n",
      "  Mean Squared Error (MSE): $2,403,551,391,059.07\n",
      "  Root Mean Squared Error (RMSE): $1,550,339.12\n",
      "  Mean Absolute Error (MAE): $594,792.66\n",
      "  R² Score: 0.1837\n",
      "\n",
      "Most Important Features:\n",
      "                       Feature  Importance\n",
      "                   Area (SqFt)    0.588528\n",
      "Neighborhood_Pacific Palisades    0.085720\n",
      "    Property Type_MULTI_FAMILY    0.070408\n",
      "                     Bathrooms    0.069523\n",
      "   Property Type_SINGLE_FAMILY    0.059344\n",
      "                      Bedrooms    0.038340\n",
      "      Neighborhood_Los Angeles    0.034059\n",
      "           Property Type_CONDO    0.028981\n",
      "      Neighborhood_Canoga Park    0.007641\n",
      "    Neighborhood_Beverly Hills    0.006027\n",
      "\n",
      "Model building completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "target = \"Sold Price\"\n",
    "X_train = train_data.drop(columns=[target])\n",
    "y_train = train_data[target]\n",
    "X_test = test_data.drop(columns=[target])\n",
    "y_test = test_data[target]\n",
    "\n",
    "numeric_features = [\"Bedrooms\", \"Bathrooms\", \"Area (SqFt)\"]\n",
    "categorical_features = [\"Property Type\", \"Neighborhood\"]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200,          \n",
    "    max_depth=20,              \n",
    "    min_samples_split=5,        \n",
    "    min_samples_leaf=2,         \n",
    "    random_state=40,            \n",
    "    n_jobs=-1                   \n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', rf_model)\n",
    "])\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"Model training completed!\")\n",
    "\n",
    "print(\"Making predictions...\")\n",
    "y_train_pred = model_pipeline.predict(X_train)\n",
    "y_test_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, dataset_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Set Performance:\")\n",
    "    print(f\"  Mean Squared Error (MSE): ${mse:,.2f}\")\n",
    "    print(f\"  Root Mean Squared Error (RMSE): ${rmse:,.2f}\")\n",
    "    print(f\"  Mean Absolute Error (MAE): ${mae:,.2f}\")\n",
    "    print(f\"  R² Score: {r2:.4f}\")\n",
    "    \n",
    "    return {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2}\n",
    "\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred, \"Training\")\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred, \"Testing\")\n",
    "\n",
    "feature_names = (numeric_features + \n",
    "                list(model_pipeline.named_steps['preprocessor']\n",
    "                    .named_transformers_['cat']\n",
    "                    .named_steps['encoder']\n",
    "                    .get_feature_names_out(categorical_features)))\n",
    "\n",
    "feature_importance = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nMost Important Features:\")\n",
    "print(feature_importance_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nModel building completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model descibe the steps taken to acheve the best rusults possiable \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when I began i started with only 50 tress, but i realized that was too few and it was overfitting the model, so I bumped it up to 400 trees and things were fine.\n",
    "I asked chatgpt about it and it explained that with the amount of rows in the data, 200 trees was sufficient, so i stuck with that.\n",
    "\n",
    "I read that with less than 5k rows, the max depth should be 5-10, so i just chose 7 and stuck with it.\n",
    "\n",
    "I just started at 1 for the random state and started playing with it and hit 45 as the highest percentage of 82 and stuck with it, but really it doesnt\n",
    "matter, its just something used to ensure the data is reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer faceing Document provide summery of finding and detail approach taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Findings and Approach\n",
    "We analyzed passenger survivability on the titanic using a dataset provided from Kaggle. Our analysis aimed to identify which factors most influenced survival and to develop a predictive model. From the data:\n",
    "Sex is the strongest predictor: Females had a roughly 75% survival rate, while males were around 20%.\n",
    "\n",
    "\n",
    "Passenger class and age are also significant: First-class passengers had a survival rate of ~63%, decreasing with lower classes, and children aged 10 and under had about a 60% chance of survival.\n",
    "\n",
    "\n",
    "Bias in survival: The data reflects historical prioritization of women, children, and wealthier passengers.\n",
    "\n",
    "\n",
    "Cleaning the data:\n",
    "Missing values were filled in\n",
    "Categorical variables were converted\n",
    "New features were created, like family size and title.\n",
    "Useless features were removed\n",
    "\n",
    "\n",
    "For modeling:\n",
    "Initial testing with logistic regression had 80% accuracy.\n",
    "Moved to Random Forest model\n",
    "Model optimization:\n",
    "\n",
    "\n",
    "Number of trees: Increased from 50 (overfitting) to 200.\n",
    "\n",
    "\n",
    "Max depth: Set to 7, as it is a small dataset\n",
    "Set a random state to make sure the results are reproduced\n",
    "Outcome: The Random Forest model gave an accuracy of approximately 82%. Therefore, it can be reliably used to predict whether a passenger will survive or not, given a new set of data..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code, This should be self susficent and require only your model pramaters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(params):\n",
    "    params = params.reindex(columns=X.columns, fill_value=0)\n",
    "    results = model.predict(params)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
